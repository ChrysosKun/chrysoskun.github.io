---
title: Docker基础
categories: [linux,container]
comments: true
---

Docker容器与虚拟机类似，但二者在原理上不同。容器是将[操作系统层虚拟化](https://zh.wikipedia.org/wiki/作業系統層虛擬化)，虚拟机则是虚拟化硬件，因此容器更具有便携性、高效地利用服务器。 容器更多的用于表示 软件的一个标准化单元。由于容器的标准化，因此它可以无视基础设施的差异，部署到任何一个地方。另外，Docker也为容器提供更强的业界的隔离兼容。

![image](https://cdn.jsdelivr.net/gh/chrysoskun/chrysoskun.github.io/assets/img/docker/image12.png)

* TOC
{:toc}
## 安装

```shell
# 卸载旧版本
sudo yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-selinux \
                  docker-engine-selinux \
                  docker-engine

# 安装依赖包
yum install -y yum-utils

# 安装 Docker CE
# 更新 yum 软件源缓存，并安装 docker-ce
yum makecache fast
yum install docker-ce docker-ce-cli containerd.io

# 使用阿里云的源
yum-config-manager --add-repo\ 
http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

# 阿里云加速器
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
	"registry-mirrors": ["https://xxxxxxxx.mirror.aliyuncs.com"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker

# 启动 Docker CE
sudo systemctl enable docker
sudo systemctl start docker

# 建立 docker 用户组
# 默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。
# 而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。
# 出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。
# 因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组

# 建立 docker 组
sudo groupadd docker

# 将当前用户加入 docker 组
sudo usermod -aG docker $USER

# 文档
docker run -d -p 4000:80 --restart=always\
--name praker_practice01\
ccr.ccs.tencentyun.com/dockerpracticesig/docker_practice
```



## 常用命令



```shell
# 获取镜像
docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签]
比如：

$ docker pull centos:7
7: Pulling from library/centos
75f829a71a1c: Pull complete 
Digest: sha256:19a79828ca2e505eaee0ff38c...
Status: Downloaded newer image for centos:7
docker.io/library/centos:7

# 查看镜像
docker images

# 运行
docker run -it centos /bin/bash

# 删除镜像
docker rmi -f 镜像ID

# 查看容器
docker ps 查看正在运行的容器
				-a 查看运行过的容器

# 删除容器
docker rm -f 容器ID

# 重新进入容器
docker exec -it 容器ID /bin/bash  进入容器打开新的bash
docker attach 容器ID              进入容器退出时状态

# 将文件从docker拷贝到主机
docker cp 容器ID:/home/test.cpp /home/

# 查看log
docker logs -f -t --tail 10 容器ID
```



## 练习



### 部署nginx

1.搜索镜像search

2.下载镜像pull

3.运行测试

```shell
docker pull nginx
docker run -d --name nginx01 -p 3344:80 nginx
# -d 后台运行
# --name 给容器命名
# -p 宿主机端口，容器内部端口
curl localhost:3344
docker ps
```

端口暴露的概念：

![image](https://cdn.jsdelivr.net/gh/chrysoskun/chrysoskun.github.io/assets/img/docker/image1.png)

### 部署tomcat

```shell
# 官方使用
docker run -it --rm tomcat:9.0

# 之前的后台启动，停止了容器以后，容器还可以查到
# docker run -it --rm, 一般用来测试，用完就删

# 下载启动
docker pull tomcat

# 启动运行
docker run -d -p 3355:8080 --name tomcat01 tomcat

# 进入容器
docker exec -it tomcat01 /bin/bash
```



### 部署es + kibana



```shell
# es暴露的端口很多

# es十分耗内存

# es的数据一般需要放置到安全目录挂载

docker run -d --name elasticsearch\
--net somenetwork\
-p 9200:9200\
-p 9300:9300\
-e "discovery.type=single-node" elasticsearch:tag

# --net somenetwork 网络配置（暂时不用）

# 启动

docker run -d --name elasticsearch\
-p 9200:9200\
-p 9300:9300\
-e "discovery.type=single-node" elasticsearch：7.6.2

[root@chrysoskun ~]# curl localhost:9200
{
  "name" : "832ca327af75",
  "cluster_name" : "docker-cluster",
  "cluster_uuid" : "RYP_xJFESh66x_pz5c7ybg",
  "version" : {
    "number" : "7.6.2",
    "build_flavor" : "default",
    "build_type" : "docker",
    "build_hash" : "ef48eb35cf30adf4db14086e8aabd07ef6fb113f",
    "build_date" : "2020-03-26T06:34:37.794943Z",
    "build_snapshot" : false,
    "lucene_version" : "8.4.0",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "You Know, for Search"
}

#docker stats 查看状态

# 增加内存限制，修改配置文件 -e 环境配置修改
docker run -d\
--name elasticsearch02\
-p 9200:9200\
-p 9300:9300\
-e "discovery.type=single-node"\
-e ES_JAVA_OPTS="-Xms64m -Xmx512m" elasticsearch：7.6.2

```

使用kibana连接es

![image](https://cdn.jsdelivr.net/gh/chrysoskun/chrysoskun.github.io/assets/img/docker/image2.png)

## 可视化

```shell
docker run -d -p 8088:9000 --restart=always\
-v /var/run/docker.sock:/var/run/docker.sock\
--privileged=true portainer/portainer
```

![image](https://cdn.jsdelivr.net/gh/chrysoskun/chrysoskun.github.io/assets/img/docker/image3.png)

![image](https://cdn.jsdelivr.net/gh/chrysoskun/chrysoskun.github.io/assets/img/docker/image4.png)

------

## 容器数据卷

### 什么是容器数据卷

docker的理念回顾

`将应用和环境打包成一个镜像`

- 如果数据都在容器中，容器删除，数据就会丢失。需求：数据可以持久化
- MySQL,容器删除=删库跑路。需求：MySQL数据可以存储到本地
- 容器之间可以有一个共享技术。Docker容器产生的数据可以同步到本地
- 卷技术：目录的挂载，将容器目录挂载到Linux

![image](https://cdn.jsdelivr.net/gh/chrysoskun/chrysoskun.github.io/assets/img/docker/image5.png)

### 使用数据卷

**直接使用命令行来挂载 -v**

```shell
docker run -it -v 主机目录:容器内目录

#测试
docker run -it -v /home/seshi:/home centos /bin/bash

#启动起来使用docker inspect 容器ID
```

`停止容器数据依然是同步的`



### 实战：安装MySQL

```shell
# 获取镜像
docker pull mysql:5.7

# 运行容器，需要做数据挂载
# 安装启动MySQL需要配置密码
# 官方测试：docker run --name \
#         some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw\
#         -d mysql:tag

# 参数
-d 后台运行
-p 端口映射
-v 卷挂载
-e 环境配置
--name 容器名字
awesome
# 启动
docker run -d -p 3310:3306 --restart=always\
-v /home/mysql/conf:/etc/mysql/conf.d\
-v /home/mysql/data:/var/lib/mysql\
-e MYSQL_ROOT_PASSWORD=mymysql\
--name mysql01 mysql:5.7
```



## 具名挂载和匿名挂载

```shell
# 匿名挂载
-v 容器路径
docker run -d -P --name nginx01 -V /etc/nginx nginx

# 查看所有的volume的情况
docker volume ls

[root@chrysoskun ~] docker volume ls
DRIVER              VOLUME NAME
local               39fe6e4095e817bf1...

# 这种就是匿名挂载，在 -v 只写了容器内的路径，没有写容器外的路径

# 具名挂载

docker run -d -P --name nginx02\
-v juming-nginx:/etc/nginx nginx

[root@chrysoskun ~] docker volume ls
DRIVER              VOLUME NAME
local               39fe6e4095e817bf12c2c...
local               juming-nginx

# -v 卷名：容器内路径

# 查看这个卷
[root@chrysoskun ~] docker volume inspect juming-nginx
[
    {
        "CreatedAt": "2020-09-07T11:00:26+08:00",
        "Driver": "local",
        "Labels": null,
        "Mountpoint": "/var/lib/docker/volumes/juming-nginx/_data",
        "Name": "juming-nginx",
        "Options": null,
        "Scope": "local"
    }
]
```

- 所有的docker容器内的卷，没有指定目录的情况下都在 /var/lib/docker/volumes/xxxx/_data
- 通过具名挂载可以方便的找到一个卷

------



## 初识Dockerfile

**创建镜像时挂载**

Dockerfile就是用来构建docker镜像的构建文件，命令脚本。

通过脚本可以生成镜像，镜像是一层一层的，脚本一个个的命令，每个命令都是一层。

```shell
# 创建一个dockerfile文件，名字可以随机，建议Dockerfile
# 文件中的内容 指令（大写）参数

FROM centos

VOLUME ["volume01","volume02"]      # 此为匿名挂载

CMD echo "----------end----------"

CMD /bin/bash

# 这里每个命令就像镜像的一层

# 构建
docker build -f dockerfile01 -t langyaya/centos .

[root@chrysoskun] docker build -f dockerfile01 -t langyaya/centos .
Sending build context to Docker daemon  2.048kB
Step 1/4 : FROM centos
 ---> 0d120b6ccaa8
Step 2/4 : VOLUME ["volume01","volume02"]
 ---> Running in e501cffb694a
Removing intermediate container e501cffb694a
 ---> f336ad7bfb33
Step 3/4 : CMD echo "----------end----------"
 ---> Running in e903ab3d8fa2
Removing intermediate container e903ab3d8fa2
 ---> 3a820a23a5f7
Step 4/4 : CMD /bin/bash
 ---> Running in 9eeb562caf95
Removing intermediate container 9eeb562caf95
 ---> 2d5ac7cc876b
Successfully built 2d5ac7cc876b
Successfully tagged langyaya/centos:latest

[root@chrysoskun docker-volume] docker images

# 构建完成

# 用自己构建的镜像创建容器
docker run -it 2d5ac7cc876b

[root@22e5a20e1e9c /] ls -l
total 56
lrwxrwxrwx  1 root root    7 May 11  2019 bin -> usr/bin
drwxr-xr-x  5 root root  360 Sep  7 06:43 dev
drwxr-xr-x  1 root root 4096 Sep  7 06:43 etc
drwxr-xr-x  2 root root 4096 May 11  2019 home
lrwxrwxrwx  1 root root    7 May 11  2019 lib -> usr/lib
lrwxrwxrwx  1 root root    9 May 11  2019 lib64 -> usr/lib64
drwx------  2 root root 4096 Aug  9 21:40 lost+found
drwxr-xr-x  2 root root 4096 May 11  2019 media
drwxr-xr-x  2 root root 4096 May 11  2019 mnt
drwxr-xr-x  2 root root 4096 May 11  2019 opt
dr-xr-xr-x 89 root root    0 Sep  7 06:43 proc
dr-xr-x---  2 root root 4096 Aug  9 21:40 root
drwxr-xr-x 11 root root 4096 Aug  9 21:40 run
lrwxrwxrwx  1 root root    8 May 11  2019 sbin -> usr/sbin
drwxr-xr-x  2 root root 4096 May 11  2019 srv
dr-xr-xr-x 13 root root    0 Sep  7 06:43 sys
drwxrwxrwt  7 root root 4096 Aug  9 21:40 tmp
drwxr-xr-x 12 root root 4096 Aug  9 21:40 usr
drwxr-xr-x 20 root root 4096 Aug  9 21:40 var
drwxr-xr-x  2 root root 4096 Sep  7 06:43 volume01
drwxr-xr-x  2 root root 4096 Sep  7 06:43 volume02

# 发现卷已经挂载

```

## 数据卷容器

两个容器之间实现数据同步

![image](https://cdn.jsdelivr.net/gh/chrysoskun/chrysoskun.github.io/assets/img/docker/image6.png)

```shell
# 通过刚才制作的镜像启动3个容器

# 启动docker01

[root@chrysoskun ~] docker run -it  --name docker01 langyaya/centos

# docker02继承docker01,前者为子类，后者为父类

# --volumes-from 实现容器间的数据共享

# docker01为数据卷容器

[root@chrysoskun ~] docker run -it\
			   --name docker02\
		       --volumes-from docker01 langyaya/centos

# 创建docker03

[root@chrysoskun ~] docker run -it\
--name docker03\
--volumes-from docker01 langyaya/centos

# 删除docker01,docker02和docker03数据还在
```

![image](https://cdn.jsdelivr.net/gh/chrysoskun/chrysoskun.github.io/assets/img/docker/image7.png)

### 多个MySQL实现数据库共享

```shell
docker run -d -p 3310:3306 --restart=always
-v /home/mysql/conf:/etc/mysql/conf.d
-v /home/mysql/data:/var/lib/mysql 
-e MYSQL_ROOT_PASSWORD=mymysql --name mysql01 mysql:5.7

docker run -d -p 3310:3306 --restart=always 
-e MYSQL_ROOT_PASSWORD=mymysql
--name mysql02 --volumes-from mysql01 mysql:5.7
```

### 结论

容器之间配置信息的传递，数据卷容器的生命周期一直持续到没有容器使用为止。但是一旦持久化到本地，这个时候，本地数据是不会删除的。

------



## DockerFile

### DockerFile介绍

构建步骤：

1. 编写一个DockerFile文件
2. docker build 构建成为一个镜像
3. docker run 运行镜像
4. docker push 发布镜像（DockerHub、阿里云镜像仓库）

![image](https://cdn.jsdelivr.net/gh/chrysoskun/chrysoskun.github.io/assets/img/docker/image8.png)

### DockerFile指令

```shell
FROM              # 基础镜像，一切从这里开始构建
MAINTAINER        # 镜像是谁写的，姓名+邮箱
RUN               # 镜像构建时需要运行的命令
ADD               # 添加内容
WORKDIR           # 镜像的工作目录
VOLUME            # 挂载的目录
EXPOSE            # 暴露端口配置
CMD               # 指定这个容器启动时要运行的命令（替代）
ENTRYPOINT        # 指定这个容器启动时要运行的命令（追加）
ONBUILD           # 当构建一个被继承DockerFile就会运行ONBUILD的指令。触发指令
COPY              # 将文件拷贝到镜像中
ENV               # 构建的时候设置环境变量
```

### 使用DockerFile创建镜像

```shell
# 编写DockerFile文件
[root@chrysoskun dockerfile] cat dockerfile-mycentos 
FROM centos
MAINTAINER langyaya<1223456@qq.com>

ENV MYPATH /home
WORKDIR $MYPATH
RUN yum -y install net-tools
EXPOSE 80

CMD echo $MYPATH
CMD echo "---------end--------"
CMD /bin/bash

# 构建
[root@chrysoskun dockerfile] docker build -f dockerfile-mycentos -t langyaya/centos .

```



## Docker网络

### 理解docker0

```shell
[root@chrysoskun ~] ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500
	qdisc pfifo_fast state UP qlen 1000
    link/ether 00:16:3e:12:91:81 brd ff:ff:ff:ff:ff:ff
    inet 172.28.139.188/20 brd 172.28.143.255 scope global dynamic eth0
       valid_lft 314836728sec preferred_lft 314836728sec
3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP 
    link/ether 02:42:43:38:13:5e brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
```

*Docker是如何处理容器网络访问的？*

### ip addr测试

```shell
# 查看容器内部网络地址ip addr,会得到一个 eth0@if61 ip地址。docker分配的。

[root@chrysoskun ~] docker exec -it ubuntu01 ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state
	UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
60: eth0@if61: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 
	qdisc noqueue state UP group default 
    link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.5/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever

# Linux能不能ping通容器内部？
[root@chrysoskun ~] ping 172.17.0.5
PING 172.17.0.5 (172.17.0.5) 56(84) bytes of data.
64 bytes from 172.17.0.5: icmp_seq=1 ttl=64 time=0.059 ms
64 bytes from 172.17.0.5: icmp_seq=2 ttl=64 time=0.071 ms
64 bytes from 172.17.0.5: icmp_seq=3 ttl=64 time=0.061 ms
64 bytes from 172.17.0.5: icmp_seq=4 ttl=64 time=0.072 ms
64 bytes from 172.17.0.5: icmp_seq=5 ttl=64 time=0.059 ms
^C
--- 172.17.0.5 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 3999ms
rtt min/avg/max/mdev = 0.059/0.064/0.072/0.009 ms

# Linux可以ping通docker容器内部
```

原理:

我们每启动一个docker容器，docker就会给docker容器分配一个ip,只要安装了docker,就会有一个网卡docker0,桥接模式，使用veth-pair技术

### 再次ip addr测试

```shell
[root@chrysoskun ~] ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue
	state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500
	qdisc pfifo_fast state UP qlen 1000
    link/ether 00:16:3e:12:91:81 brd ff:ff:ff:ff:ff:ff
    inet 172.28.139.188/20 brd 172.28.143.255 scope 
    	global dynamic eth0
       valid_lft 314835216sec preferred_lft 314835216sec
3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500
	qdisc noqueue state UP 
    link/ether 02:42:43:38:13:5e brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: veth4e0627a@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500
	qdisc noqueue master docker0 state UP 
    link/ether a2:6a:e8:6b:5f:0b brd ff:ff:ff:ff:ff:ff link-netnsid 0
7: veth14825e1@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500
	qdisc noqueue master docker0 state UP 
    link/ether 72:59:bb:db:b9:ca brd ff:ff:ff:ff:ff:ff link-netnsid 1
9: vetha1c0238@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500
	qdisc noqueue master docker0 state UP 
    link/ether 2e:75:88:91:df:cb brd ff:ff:ff:ff:ff:ff link-netnsid 2
51: veth856e668@if50: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500
	qdisc noqueue master docker0 state UP 
    link/ether 52:01:0f:50:d2:2c brd ff:ff:ff:ff:ff:ff link-netnsid 5
61: veth22d121d@if60: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500
	qdisc noqueue master docker0 state UP 
    link/ether aa:44:1d:72:c6:cd brd ff:ff:ff:ff:ff:ff link-netnsid 3
```

- 发现容器带来的网卡都是一对对的 
- veth-pair 就是一对的虚拟设备接口，它们都是成对出现的，一段连着协议，一段彼此相连 
- 正因为有这个特性，veth-pair 充当一个桥梁，连接各种网络设备 
- OpenStac,Docker容器直接的连接，OVS的连接，都是使用veth-pair技术

### docker0与容器的网络关系

![image](https://cdn.jsdelivr.net/gh/chrysoskun/chrysoskun.github.io/assets/img/docker/image9.png)

*tomcat01和tomcat02是公用的一个路由器—dcoker0。所有的容器在不指定网络的情况下，都是docker0路由的，docker会给容器分配一个默认的可用IP*

**Docker使用的是Linux桥接，宿主机中是一个Docker容器的网桥—docker0**

![image](https://cdn.jsdelivr.net/gh/chrysoskun/chrysoskun.github.io/assets/img/docker/image10.png)

*Docker中的所有网络接口都是虚拟的。虚拟的转发效率高（内网传递文件）。容器删除，对应网桥一对就没了。*

### —link

不用ip,通过容器名能否ping通?

```shell
# 通过 --link 解决

[root@chrysoskun ~] docker run -d -P --name tomcat03 --link tomcat02 tomcat
3e22c4ca749d2e301183891b37edcba0ef06eafa9050ffc1ad7d3a45004303e6

[root@chrysoskun ~] docker exec -it tomcat03 ping tomcat02
PING tomcat02 (172.17.0.4) 56(84) bytes of data.
64 bytes from tomcat02 (172.17.0.4): icmp_seq=1 ttl=64 time=0.111 ms
64 bytes from tomcat02 (172.17.0.4): icmp_seq=2 ttl=64 time=0.089 ms
64 bytes from tomcat02 (172.17.0.4): icmp_seq=3 ttl=64 time=0.074 ms
64 bytes from tomcat02 (172.17.0.4): icmp_seq=4 ttl=64 time=0.083 ms
64 bytes from tomcat02 (172.17.0.4): icmp_seq=5 ttl=64 time=0.074 ms
64 bytes from tomcat02 (172.17.0.4): icmp_seq=6 ttl=64 time=0.077 ms
^C
--- tomcat02 ping statistics ---
6 packets transmitted, 6 received, 0% packet loss, time 1003ms
rtt min/avg/max/mdev = 0.074/0.084/0.111/0.016 ms

# 思考：能反向ping通吗？
[root@chrysoskun ~] docker exec -it tomcat02 ping tomcat03
ping: tomcat03: Name or service not known
```

探究：inspect

```shell
[root@chrysoskun ~] docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
bb34eff8833e        bridge              bridge              local
579aa6611400        host                host                local
815245f2af45        none                null                local
[root@chrysoskun ~] docker inspect bb34eff8833e
[
    {
        "Name": "bridge",
        "Id": "bb34eff8833e216b530810a0af9f788f0e8a94020e7da4fc8a5019d",
        "Created": "2020-09-07T15:21:50.118147169+08:00",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": null,
            "Config": [
                {
                    "Subnet": "172.17.0.0/16",
                    "Gateway": "172.17.0.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Containers": {
            "3e22c4ca749d2e301183891b37edcba0ef06eafa905004303e6": {
                "Name": "tomcat03",
                "EndpointID": "b8911d386d97d8e98af248c1edff0858c74683eabf8ba3c3",
                "MacAddress": "02:42:ac:11:00:05",
                "IPv4Address": "172.17.0.5/16",
                "IPv6Address": ""
            },
            "93d669f36baa0ad065bcf9c35a073ff7f78876f2a8fa6a8b3e3e": {
                "Name": "praker_practice01",
                "EndpointID": "e729c812206f31d4cf5062c7a1ad73bda4d",
                "MacAddress": "02:42:ac:11:00:03",
                "IPv4Address": "172.17.0.3/16",
                "IPv6Address": ""
            },
            "96eef8333179554bb32458732b2b04f96e243ed0d96ff4a6": {
                "Name": "portainer-ce01",
                "EndpointID": "5bdae6734e124f2fc396eb1a684093e601ab4",
                "MacAddress": "02:42:ac:11:00:02",
                "IPv4Address": "172.17.0.2/16",
                "IPv6Address": ""
            },
            "c3f19955131a956b88cab87def990059f36e3346983ac4988": {
                "Name": "tomcat02",
                "EndpointID": "c0a72c33a66bd021a23162f8cb41ab4b7",
                "MacAddress": "02:42:ac:11:00:04",
                "IPv4Address": "172.17.0.4/16",
                "IPv6Address": ""
            }
        },
        "Options": {
            "com.docker.network.bridge.default_bridge": "true",
            "com.docker.network.bridge.enable_icc": "true",
            "com.docker.network.bridge.enable_ip_masquerade": "true",
            "com.docker.network.bridge.host_binding_ipv4": "0.0.0.0",
            "com.docker.network.bridge.name": "docker0",
            "com.docker.network.driver.mtu": "1500"
        },
        "Labels": {}
    }
]
```

tomcat03在本地配置了tomcat02的配置

```shell
# 查看 hosts 配置，发现原理
[root@chrysoskun ~] docker exec -it tomcat03 cat /etc/hosts
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.17.0.4      tomcat02 c3f19955131a
172.17.0.5      3e22c4ca749
```

- —link就是在hosts配置中写入了tomcat03的ip和名字
- 现在使用docker0已经不建议使用—link了
- docker0问题：不支持容器名连接访问

### 自定义网络

```shell
[root@chrysoskun ~] docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
bb34eff8833e        bridge              bridge              local
579aa6611400        host                host                local
815245f2af45        none                null                local
```

网络模式:

1. bridge:桥接（docker0默认)
2. none:不配置网络
3. host:和主机共享网络
4. container:容器网络连通（局限性很大）

**测试**

```shell
# 我们启动tomcat不加 --net 参数 默认是brige,就是docker0
[root@chrysoskun ~] docker run -d -P --name tomcat01 tomcat
[root@chrysoskun ~] docker run -d -P --name tomcat01 --net bridge tomcat
# 上面两个命令相等

# docker0特点：默认，域名不能访问，--link可以打通连接（不建议）

# 我们可以自定义一个网络
# --driver bridge
# --subnet 192.168.0.0/16
# --gateway 192.168.0.1

[root@chrysoskun ~] docker network create --help

Usage:  docker network create [OPTIONS] NETWORK

Create a network

Options:
      --attachable           Enable manual container attachment
      --config-from string   The network from which copying the configuration
      --config-only          Create a configuration only network
  -d, --driver string        Driver to manage the Network (default "bridge")
      --gateway strings      IPv4 or IPv6 Gateway for the master subnet
      --ingress              Create swarm routing-mesh network
      --internal             Restrict external access to the network
      --ip-range strings     Allocate container ip from a sub-range
      --ipam-driver string   IP Address Management Driver (default "default")
      --ipam-opt map         Set IPAM driver specific options (default map[])
      --ipv6                 Enable IPv6 networking
      --label list           Set metadata on a network
  -o, --opt map              Set driver specific options (default map[])
      --scope string         Control the network's scope
      --subnet strings       Subnet in CIDR format that 
      represents a network segment

[root@chrysoskun ~] docker network create\
				--driver bridge\
				--subnet 192.168.0.0/16\
				--gateway 192.168.0.1 mynet
60c1cfbb7e9e40d627ef8b16619cb52bc44db266f97bd9b3f5cbc36f2b01607c

[root@chrysoskun ~] docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
bb34eff8833e        bridge              bridge              local
579aa6611400        host                host                local
60c1cfbb7e9e        mynet               bridge              local
815245f2af45        none                null                local

# 自定义网络创建完成
[root@chrysoskun ~] docker network inspect mynet
[
    {
        "Name": "mynet",
        "Id": "60c1cfbb7e9e40d627ef8b16619cb52bc44db266f901607c",
        "Created": "2020-09-14T19:31:51.172097261+08:00",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": {},
            "Config": [
                {
                    "Subnet": "192.168.0.0/16",
                    "Gateway": "192.168.0.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Containers": {},
        "Options": {},
        "Labels": {}
    }
]

# 使用自己的网络进行测试

[root@chrysoskun ~] docker run -d -P --name tomcat-net-01 --net mynet tomcat
d4f4891d9885cc843cfd3e7e4c2556067b44a918ff3d4de6c64b4268d4a561c5
[root@chrysoskun ~] docker run -d -P --name tomcat-net-02 --net mynet tomcat
fb5b6acfee503b02b8dc8168da32a6ab25e98566ece026e38ff327d03ab0cc55
[root@chrysoskun ~] docker network inspect mynet
"Containers": {
            "d4f4891d9885cc843cfd3e7e4c2556067b44a914a561c5": {
                "Name": "tomcat-net-01",
                "EndpointID": "3f0fa235b61faf72a60f3406b023e3d4aaa846",
                "MacAddress": "02:42:c0:a8:00:02",
                "IPv4Address": "192.168.0.2/16",
                "IPv6Address": ""
            },
            "fb5b6acfee503b02b8dc8168da32a6ab25e98566ece07d03ab0cc55": {
                "Name": "tomcat-net-02",
                "EndpointID": "2c00dd3d5632882b26db18b0df74682aaafefbb2",
                "MacAddress": "02:42:c0:a8:00:03",
                "IPv4Address": "192.168.0.3/16",
                "IPv6Address": ""
            }
        }

# 再次测试ping连接
[root@chrysoskun ~] docker exec -it tomcat-net-01 ping tomcat-net-02
PING tomcat-net-02 (192.168.0.3) 56(84) bytes of data.
64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=1 ttl=64 time=0.072 ms
64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=2 ttl=64 time=0.084 ms
64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=3 ttl=64 time=0.091 ms
64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=4 ttl=64 time=0.083 ms
64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=5 ttl=64 time=0.072 ms
^C
--- tomcat-net-02 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 4ms
rtt min/avg/max/mdev = 0.072/0.080/0.091/0.010 ms

# 现在不用 --link 也可以ping名字了
[root@chrysoskun ~] docker exec -it tomcat-net-01 ping tomcat-net-02
PING tomcat-net-02 (192.168.0.3) 56(84) bytes of data.
64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=1 ttl=64 time=0.072 ms
64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=2 ttl=64 time=0.084 ms
64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=3 ttl=64 time=0.091 ms
64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=4 ttl=64 time=0.083 ms
64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=5 ttl=64 time=0.072 ms
^C
--- tomcat-net-02 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 4ms
rtt min/avg/max/mdev = 0.072/0.080/0.091/0.010 ms
```

*自定义的网络已经帮我们维护好了对应关系，推荐使用自定义网络*

**好处**：

- redis-不同集群使用不同网络，保证集群是安全和健康的
- mysql-不同集群使用不同网络，保证集群是安全和健康的

**网络连通**

```shell
[root@chrysoskun ~] docker network --help

Usage:  docker network COMMAND

Manage networks

Commands:
  connect     Connect a container to a network

[root@chrysoskun ~] docker network connect --help

Usage:  docker network connect [OPTIONS] NETWORK CONTAINER

Connect a container to a network

Options:
      --alias strings           Add network-scoped alias for the container
      --driver-opt strings      driver options for the network
      --ip string               IPv4 address (e.g., 172.30.100.104)
      --ip6 string              IPv6 address (e.g., 2001:db8::33)
      --link list               Add link to another container
      --link-local-ip strings   Add a link-local address for the container

[root@chrysoskun ~] docker network connect mynet tomcat01
[root@chrysoskun ~] docker network inspect mynet
"fdfcc1355523d6028ffdd5cfe9b9a3ba504598d12e9d16f44926214730c7d28d": {
                "Name": "tomcat01",
                "EndpointID": "f5829e35be68a591b73aaa160de8ee3817866bd9c58ac0",
                "MacAddress": "02:42:c0:a8:00:04",
                "IPv4Address": "192.168.0.4/16",
                "IPv6Address": ""
            }

# 连通之后就是将tomcat01放到了mynet下
# 一个容器两个ip
# 公网ip,私网ip

[root@chrysoskun ~] docker exec -it tomcat01 ping tomcat-net-01
PING tomcat-net-01 (192.168.0.2) 56(84) bytes of data.
64 bytes from tomcat-net-01.mynet (192.168.0.2): icmp_seq=1 ttl=64 time=0.081 ms
64 bytes from tomcat-net-01.mynet (192.168.0.2): icmp_seq=2 ttl=64 time=0.074 ms
64 bytes from tomcat-net-01.mynet (192.168.0.2): icmp_seq=3 ttl=64 time=0.076 ms
64 bytes from tomcat-net-01.mynet (192.168.0.2): icmp_seq=4 ttl=64 time=0.074 ms
64 bytes from tomcat-net-01.mynet (192.168.0.2): icmp_seq=5 ttl=64 time=0.072 ms
^C
--- tomcat-net-01 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 1003ms
rtt min/avg/max/mdev = 0.072/0.075/0.081/0.008 ms
# 可ping通
```

